{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2024, 2, 28, 17, 1, 47, 335850)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02_28_2024_17_01_47.log'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\FSDS\\\\notebooks\\\\logs'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.getcwd(),\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.DimondPricePrediction.logger import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"I have just tested the things\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "division by zero\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "        a=1/0\n",
    "except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_21288\\3930040423.py\n",
      "(<class 'ZeroDivisionError'>, ZeroDivisionError('division by zero'), <traceback object at 0x00000290C5306300>)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "        a=1/0\n",
    "except Exception as e:\n",
    "        _,_,exc_tb = sys.exc_info()\n",
    "        print(exc_tb.tb_lineno)\n",
    "        print(exc_tb.tb_frame.f_code.co_filename)\n",
    "        print(sys.exc_info())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(os.path.join(\"artifacts\",\"raw.csv\")),exist_ok =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m X_train,y_train,X_test,y_test \u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m----> 2\u001b[0m                 \u001b[43mtrain_array\u001b[49m[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m      3\u001b[0m                  train_array[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m      4\u001b[0m                   test_array[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m      5\u001b[0m                    test_array[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      6\u001b[0m      )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_array' is not defined"
     ]
    }
   ],
   "source": [
    "X_train,y_train,X_test,y_test =(\n",
    "                train_array[:,:-1],\n",
    "                 train_array[:,-1],\n",
    "                  test_array[:,:-1],\n",
    "                   test_array[:,-1]\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'save_object' from 'src.DimondPricePrediction.utils.utils' (c:\\fsds\\src\\DimondPricePrediction\\utils\\utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDimondPricePrediction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_object\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'save_object' from 'src.DimondPricePrediction.utils.utils' (c:\\fsds\\src\\DimondPricePrediction\\utils\\utils.py)"
     ]
    }
   ],
   "source": [
    "from src.DimondPricePrediction.utils.utils import save_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(file_path,obj):\n",
    "    try:\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "\n",
    "        os.makedirs(dir_path,exist_ok=True)\n",
    "\n",
    "        with open(file_path,\"wb\") as file_obj:\n",
    "            pickle.dump(obj,file_obj)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise customexception(e,sys)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artifacts and configuration\n",
    "Artifacts are output of each component like data_ingestion--->Raw data \n",
    "data_trans---> preprocessed object\n",
    "model_trainer---> model object\n",
    "\n",
    "configuration in present inside each component module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_data_transformation(self, train_path, test_path):\n",
    "        try:\n",
    "            train_df=pd.read_csv(train_path)\n",
    "            test_df=pd.read_csv(test_path)\n",
    "            logging.info(\"read train and test data complete\")\n",
    "            logging.info(f'Train DataFrame Head : \\n {train_df.head().to_string()}')\n",
    "            raise customexception(e,sys)                            \n",
    "        except Exception as e:\n",
    "            logging.info(\"Exception occours in the initiate_datatransformation\")\n",
    "            raise customexception(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_data_transformation(self, train_path, test_path):\n",
    "        try:\n",
    "            train_df=pd.read_csv(train_path)\n",
    "            test_df=pd.read_csv(test_path)\n",
    "            logging.info(\"read train and test data complete\")\n",
    "            logging.info(f'Train DataFrame Head : \\n{train_df.head().to_string()}')\n",
    "            logging.info(f'Test DataFrame Head : \\n{test_df.head().to_string()}')\n",
    "\n",
    "\n",
    "            preprocessing_obj= self.get_data_transformation()  \n",
    "\n",
    "            #segregate coloumns\n",
    "            target_column_name ='price' \n",
    "            drop_columns=[target_column_name,'id']\n",
    "\n",
    "            input_feature_train_df = train_df.drop(columns=drop_columns,axis=1)\n",
    "            input_feature_test_df = test_df.drop(columns=drop_columns,axis=1)\n",
    "\n",
    "            target_feature_train_df = train_df[target_column_name]\n",
    "            target_feature_test_df=  test_df[target_column_name]\n",
    "\n",
    "            input_feature_train_arr = preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "            input_feature_test_arr = preprocessing_obj.transform(input_feature_test_df)\n",
    "\n",
    "            logging.info(\"Applying preprocessing object on train and test datasets\")\n",
    "\n",
    "            train_arr= np.c_[input_feature_train_arr,np.array(target_feature_train_df)]\n",
    "            test_arr= np.c_[input_feature_test_arr,np.array(target_feature_test_df)]\n",
    "\n",
    "            \n",
    "            save_object(\n",
    "                file_path=self.data_transformation_config.preprocessor_obj_file_path,\n",
    "                obj =preprocessing_obj\n",
    "            )\n",
    "\n",
    "            logging.info(\"Preprocessing pickle file saved\")\n",
    "\n",
    "            return (\n",
    "                train_arr,\n",
    "                test_arr\n",
    "            )\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(\"Exception occours in the initiate_datatransformation\")\n",
    "            raise customexception(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'save_object' from 'src.DimondPricePrediction.utils.utils' (c:\\fsds\\src\\DimondPricePrediction\\utils\\utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrdinalEncoder,StandardScaler\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDimondPricePrediction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_object\n\u001b[0;32m     14\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDataTransformationConfig\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     preprocessor_obj_file_path \u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martifacts\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'save_object' from 'src.DimondPricePrediction.utils.utils' (c:\\fsds\\src\\DimondPricePrediction\\utils\\utils.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.DimondPricePrediction.logger import logging\n",
    "from src.DimondPricePrediction.exception import customexception\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder,StandardScaler\n",
    "from src.DimondPricePrediction.utils.utils import save_object\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    preprocessor_obj_file_path =os.path.join('artifacts','preprocessor.pkl')\n",
    "\n",
    "class DataTransformation: \n",
    "    def __init__(self):\n",
    "        self.data_transformation_config =DataTransformationConfig()\n",
    "\n",
    "    def get_data_transformation(self):\n",
    "        try:\n",
    "            logging.info(\"Data Transformation initiated\")\n",
    "            #define which coloumn should be ordinal-encoded and which should be scaled.\n",
    "            categorical_cols = ['cut', 'color', 'clarity']\n",
    "            numerical_cols= ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "            \n",
    "            # Define the custom ranking for each ordinal variable\n",
    "            cut_categories=[ 'Fair','Good','Very Good','Premium','Ideal']\n",
    "            clarity_categories=[ 'I1','SI2','SI1','VS2', 'VS1','VVS2','VVS1', 'IF'  ]\n",
    "            color_categories=['D','E','F','G','H','I','J']\n",
    "            logging.info(\"pipeline initiated\")\n",
    "\n",
    "\n",
    "            #pipelines for transformation\n",
    "            ##Numerical pipeline\n",
    "            num_pipeline = Pipeline(\n",
    "                    steps=[\n",
    "                        ('imputer', SimpleImputer(strategy='median')),\n",
    "                        ('scaler',StandardScaler())\n",
    "                    ]\n",
    "\n",
    "            )\n",
    "            ##Categorical Pipeline\n",
    "            cat_pipeline = Pipeline(\n",
    "                    steps=[\n",
    "                        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                        ('ordinalencoder',OrdinalEncoder(categories=[cut_categories,clarity_categories,color_categories])),\n",
    "                        ('scaler',StandardScaler())\n",
    "\n",
    "                    ]\n",
    "            )\n",
    "            #transformation object\n",
    "            preprocessor=ColumnTransformer([\n",
    "                ('num_pipeline',num_pipeline,numerical_cols),\n",
    "                ('cat_pipeline',cat_pipeline,categorical_cols)\n",
    "\n",
    "            ])\n",
    "\n",
    "            return preprocessor\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(\"Exception occours in the initiate_datatransformation\")\n",
    "            raise customexception(e,sys)\n",
    "\n",
    "\n",
    "    def initialize_data_transformation(self, train_path, test_path):\n",
    "        try:\n",
    "            train_df=pd.read_csv(train_path)\n",
    "            test_df=pd.read_csv(test_path)\n",
    "            logging.info(\"read train and test data complete\")\n",
    "            logging.info(f'Train DataFrame Head : \\n{train_df.head().to_string()}')\n",
    "            logging.info(f'Test DataFrame Head : \\n{test_df.head().to_string()}')\n",
    "\n",
    "\n",
    "            preprocessing_obj= self.get_data_transformation()  \n",
    "\n",
    "            #segregate coloumns\n",
    "            target_column_name ='price' \n",
    "            drop_columns=[target_column_name,'id']\n",
    "\n",
    "            input_feature_train_df = train_df.drop(columns=drop_columns,axis=1)\n",
    "            input_feature_test_df = test_df.drop(columns=drop_columns,axis=1)\n",
    "\n",
    "            target_feature_train_df = train_df[target_column_name]\n",
    "            target_feature_test_df=  test_df[target_column_name]\n",
    "\n",
    "            input_feature_train_arr = preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "            input_feature_test_arr = preprocessing_obj.transform(input_feature_test_df)\n",
    "\n",
    "            logging.info(\"Applying preprocessing object on train and test datasets\")\n",
    "\n",
    "            train_arr= np.c_[input_feature_train_arr,np.array(target_feature_train_df)]\n",
    "            test_arr= np.c_[input_feature_test_arr,np.array(target_feature_test_df)]\n",
    "\n",
    "            \n",
    "            save_object(\n",
    "                file_path=self.data_transformation_config.preprocessor_obj_file_path,\n",
    "                obj =preprocessing_obj\n",
    "            )\n",
    "\n",
    "            logging.info(\"Preprocessing pickle file saved\")\n",
    "\n",
    "            return (\n",
    "                train_arr,\n",
    "                test_arr\n",
    "            )\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(\"Exception occours in the initiate_datatransformation\")\n",
    "            raise customexception(e,sys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
